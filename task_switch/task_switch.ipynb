{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconmenti this line and replace 'key.txt' with your key\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = open(\"key.txt\").read().strip() # run this code before engaging LLM model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor # the tools needed to create a sort of chain with directives\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools import StructuredTool\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# === TOOL DEFINITIONS ===\n",
    "def search_calendar_events(title: Optional[str]=None, datetime_range_lowerbound: Optional[str]=None) -> List[str]:\n",
    "    \"\"\n",
    "    return [f\"Meeting: {title or 'Team Sync'} scheduled at {datetime_range_lowerbound or 'Tomorrow 10 AM'}\"]\n",
    "\n",
    "def search_spotify_podcasts(topic: str) -> List[str]:\n",
    "    return [f\"Podcast on {topic}: 'Mindful Moments'\", f\"Podcast on {topic}: 'Daily Focus'\"]\n",
    "\n",
    "def get_stock_history(ticker: str, range: Optional[str] = \"1d\") -> Dict[str, Any]:\n",
    "    return {\"ticker\": ticker, \"range\": range, \"price_change\": \"+2.3%\", \"trend\": \"upward\"}\n",
    "\n",
    "calendar_tool = StructuredTool.from_function( # from_function allow the model understand the parameters of the tool, without writing them by hand\n",
    "    name='search_calendar_events',\n",
    "    func=search_calendar_events,\n",
    "    description='Search calendar events with optional filters'\n",
    ")\n",
    "\n",
    "spotify_tool = StructuredTool.from_function(\n",
    "    name='search_spotify_podcasts',\n",
    "    func=search_spotify_podcasts,\n",
    "    description='Search Podcasts'\n",
    ")\n",
    "\n",
    "stock_history_tool = StructuredTool.from_function(\n",
    "    name='get_stock_history',\n",
    "    func=get_stock_history,\n",
    "    description='Search Stock data'\n",
    ")\n",
    "\n",
    "TOOLS = [calendar_tool,spotify_tool,stock_history_tool]\n",
    "\n",
    "\n",
    "System_message = f\"\"\"\n",
    "You are an intelligent multi-domain assistant capable of understanding natural language and switching seamlessly between tasks.\n",
    "Your primary goal is to help the user by reasoning through the conversation, deciding when to use available tools, and responding in a natural and structured way.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ TOOL USAGE GUIDELINES\n",
    "\n",
    "* **Always prefer using a tool when** the userâ€™s request clearly maps to one of the available tools.\n",
    "* **Never fabricate tool arguments** â€” use only information explicitly provided by the user or logically inferred from prior context.\n",
    "* **Confirm intent** before performing high-impact or irreversible actions (e.g., scheduling, deleting, sending).\n",
    "* **If multiple tools could apply**, select the most relevant one and explain your reasoning briefly in natural language before using it.\n",
    "* If the userâ€™s request cannot be handled by any tool, provide a helpful natural-language response instead of forcing tool use.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ CONVERSATION STRUCTURE\n",
    "\n",
    "For every user input:\n",
    "\n",
    "1. **Interpret** the userâ€™s intent and reasoning clearly.\n",
    "2. **Confirm or clarify** if the request might have multiple meanings.\n",
    "3. **Use the appropriate tool** with accurate arguments.\n",
    "4. **Summarize results naturally**, integrating the toolâ€™s output into a conversational, user-friendly message.\n",
    "5. **Offer a helpful follow-up** (next possible action, suggestion, or confirmation).\n",
    "6. **Show the assistent plan** -> BEFORE calling any tool: produce a short HUMAN-FACING plan sentence prefixed with \"assistant_plan:\" describing what you will do next (example: \"assistant_plan: lets add this Event in the calender\").\n",
    "7. **Show summary** -> AFTER tool outputs, produce a clear HUMAN-FACING summary prefixed with \"assistant_summary:\" describing the result and next step (example: \"assistant_summary: Event successfully added!\").\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© TASK SWITCHING\n",
    "\n",
    "* Detect when the user **changes topic or intent**. Treat each new topic as an independent sub-task.\n",
    "* Maintain context from prior tasks only if it is explicitly relevant.\n",
    "* Avoid confusion between separate tasks (e.g., donâ€™t mix calendar and finance intents).\n",
    "* Keep tone and style consistent across task changes.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¤– RESPONSE STYLE\n",
    "\n",
    "* Be concise, polite, and natural â€” like a capable human assistant.\n",
    "* Use plain language, even when summarizing tool outputs.\n",
    "* Always ground your responses in verified data or tool results.\n",
    "* Use bullet points or short paragraphs for clarity when listing results.\n",
    "* Never expose internal reasoning or JSON tool calls to the user. The only 'Thinking' you will output are 'assistant_plan' and 'assistant_summary'.\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ ERROR HANDLING\n",
    "\n",
    "* If a tool call fails or lacks required parameters, ask the user for clarification rather than guessing.\n",
    "* If the userâ€™s input is ambiguous, ask a confirming question.\n",
    "* If the same user message includes multiple intents, handle them one by one â€” acknowledge completion before switching.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§­ OVERALL OBJECTIVE\n",
    "\n",
    "Act as a **cohesive, reasoning-driven agent** capable of:\n",
    "\n",
    "* identifying intent,\n",
    "* executing the right tool safely,\n",
    "* managing task switches gracefully,\n",
    "* and keeping the conversation clear, consistent, and helpful.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(System_message), # The systeMessage\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # chat_history (niettare contenuti dinamici nel prompt.)\n",
    "    # MessagesPlaceholder(variable_name=\"system_context\"), # system_context (niettare contenuti dinamici nel prompt.)\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") # 'agent_scratchpad' Ãˆ il meccanismo chiave per il reasoning step-by-step del modello.\n",
    "    # Contiene i risultati intermedi (tool call + return) della sessione corrente, cioÃ¨:\n",
    "    # Grazie ad esso il modello apprende anche dal 'return' delle tool invocate, e non solo dalle systemMessage.\n",
    "    # Per esempio, in questo caso quando invochiamo la search tool, il return informa il modello che \"wifi deve essere on e lower_setting_state deve essere off\"\n",
    "    # e grazie a questo return informativo(che ricordiamo non deve essere troppo lungo), l'agent_scratchpad aiuta il modello a capire di agiornare i settings prima di invocare \n",
    "    # nuovamente il 'search tool'. \n",
    "    # In parole semplici: \n",
    "    # \"il modello â€œvedeâ€ che la causa dellâ€™errore Ã¨ il Wi-Fi spento e puÃ² decidere da solo di chiamare set_system_settings() per sistemarlo.\"\"\n",
    "])\n",
    "\n",
    "\n",
    "# Crea la classe SafeConversationMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "\n",
    "class SafeConversationMemory(ConversationBufferMemory): \n",
    "    \"\"\"\n",
    "    questa classe estende ConversationBufferMemory, con lo scopo di renderla piu' â€œintelligenteâ€ sicura.\n",
    "    Enhanced memory that:\n",
    "    - Cleans malformed message content (list/dict -> string)\n",
    "    - Keeps track of system_settings status\n",
    "    - Add the actual status into the model through context by adding it in memory, so in the systemMessage you tell the model to verify in memory to make clever decisions \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print('Hit self, **kwargs')\n",
    "       \n",
    "        # self._state_actions = {'Key_Status':{ # default status. Per far capire al modello che deve updatare questi date per accedere a serch tool\n",
    "        #     'wifi':False,\n",
    "        #     'low_battery_mode':True\n",
    "        # }} \n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # ðŸ§  1ï¸âƒ£ Methods to manage pending actions (dont need those for this task)\n",
    "    # ============================================================\n",
    "\n",
    "    def set_state_action(self, name: str, key:str, value:str|bool):# ( data: self, name: str, data:dict) # we call this def from the tool def create_reservation(name,data)\n",
    "        \"\"\"Store a pending action waiting for user confirmation.\"\"\"\n",
    "        # self._state_actions[name] = data\n",
    "        self._state_actions[name][key] = value # queste value e key saranno sempre le stesse 2('wifi' e 'low_batteri_mode') e per questo non si sommeranno, ma cambieranno solo le value (true or false) \n",
    "        # Vengono aggiornate uno alla volta, in ordine del modello quando invoka l'update delle key value nella tool set_system_settings\n",
    "\n",
    "    def get_state_action(self, name: str): # controls if there are any pending actions when the model calls add_to_calender(name) Tool\n",
    "        \"\"\"Retrieve a specific pending action.\"\"\"\n",
    "        return self._state_actions.get(name) # invocando questa funzione avremo l attuale status delle key 'wifi' e 'low_batteri_mode'\n",
    "\n",
    "    # def clear_pending_action(self, name: str = None): # no need to use this function for now\n",
    "    #     \"\"\"Clear one or all pending actions.\"\"\"\n",
    "    #     if name:\n",
    "    #         self._state_actions.pop(name, None)\n",
    "    #     else:\n",
    "    #         self._state_actions.clear()\n",
    "\n",
    "    # ============================================================\n",
    "    # ðŸ§¹ 2ï¸âƒ£ Override: cleaning + injecting context for the model\n",
    "    # ============================================================\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Return cleaned chat history + pending actions as context.\"\"\"\n",
    "        raw = super().load_memory_variables(inputs) # il nome del method della classe ConversationBufferMemory, che qui estendiamo e modifichiamo per fare \n",
    "        # pulizie e aggiungere system_context in formato BaseMessage. (deve avere questo nome load_memory_variables)\n",
    "\n",
    "        print('hit load_memory_variables and inputs :',inputs) # {'input': 'where is googleplex?'}\n",
    "        # ---- Clean malformed messages (lists/dicts converted to strings)\n",
    "        if \"chat_history\" in raw:  # chat_history e' la key che abbiamo dato al nostro memoryKey quando abbiamo definito 'memory' e nel prompt\n",
    "            cleaned = []\n",
    "            print('hit chat_history')\n",
    "            for msg in raw[\"chat_history\"]:\n",
    "                if isinstance(msg, BaseMessage): # BaseMessage(HumanMessage(content:'user_input') and AIMessage(content:'ai response')) is the rigth type for our model. Its the type the model require to work without errors.\n",
    "                    # It is like that:\n",
    "                    # {'chat_history': [HumanMessage(content='I need a reservation', additional_kwargs={}, response_metadata={}), AIMessage(content='Could you please clarify what type of reservation you need? For example, is it for a restaurant, hotel, or something else? Additionally, please provide any specific details like the date, time, and number of people if applicable.', additional_kwargs={}, response_metadata={})}\n",
    "                    # so basically our memory stored in ConversationBufferMemory is stored in this way showed in HumanMessage and AIMessage\n",
    "                    msg.content = str(msg.content) # turn (list, dict) content in str type to not throw errors\n",
    "                    cleaned.append(msg)\n",
    "                elif isinstance(msg, str): # if there are some 'fully string type' value that can cause errors(like not structured baseMessage data), instead of BaseMessage structure in our\n",
    "                      # ConversationBufferMemory chat_history , then turn this string  to HumanMessage(). This 'string' bug can hapend when the model calls a tool, creating a \n",
    "                      # 'string with the name of the tool invoked', that the Model doesnt support and cannot parse.\n",
    "                      #(HumanMessage is part of baseMessage for related to the user_input. there is the exemple right up in 'if isinstance(msg, BaseMessage)'). \n",
    "                     cleaned.append(HumanMessage(content=msg)) \n",
    "            raw[\"chat_history\"] = cleaned # puliamo l object chat_history da eventuali errory\n",
    "            print('check raw ->', raw)\n",
    "\n",
    "        # ---- Add pending actions as extra conversational context. Questo previene chiamate al tool \"reservation\" quando invokiamo il tool add_to_calender\n",
    "        # inseriamo in memoria che c'e un pending tool ogni volta che il tool \"reservation\" viene invocato, e nel systemMessage diciamo specificamente\n",
    "        # al modello di non invocare nuovamente il tool \"reservation\" quando decidiamo di mettere questo \"reservatio event\" come evento nel tool \"calender\"\n",
    "        # infine ogni volta che il modello inserisce l'evento sul tool calender, il pendig state ritorna vuoto, cosicche' il processo si ripete e \n",
    "        # non ci ripeta lo stesso problema\n",
    "        # if self._state_actions: # if _state_actions not empty, cioe' se ci sono dei pending_state \n",
    "        #     context_str = (\n",
    "        #         f\"(State actions currently stored: {self._state_actions})\" # questo va a' finire nella memory nel formato humaMessage come bediamo sotto\n",
    "        #     )\n",
    "        #     print('check state_actions ->', context_str)\n",
    "        # else:\n",
    "        #     context_str = \"(No state actions stored.)\"\n",
    "        \n",
    "        # # aggiungiamo pending_actions in formato HumanMessage\n",
    "        # raw[\"system_context\"] = [HumanMessage(content=context_str)] # aggiungiamo un messaggio extra che contiene i dati di ogni turn delle values dei settngs\n",
    "        # sotto il prefisso 'system_context'(inventato da me) che poi inseriamo nel prompt/AIMessagePlaceHolder, cosi ad ogni turno il modello tiene \n",
    "        # conto dei dati ogiornati o meno di settings del turno precedente\n",
    "        # Lo stesso vale per chat_history\n",
    "        # trasformiamo il pending action in uno HumanMessage in modo che il formato sia\n",
    "        # corretto da inserire nella memory e da essere compatimile con il formato del modello.\n",
    "        # Ora  Il modello â€œvedeâ€ il pending_action nel contesto e puÃ² comportarsi di conseguenza (nei punti 5 e 6 del systemMessage diciamo al\n",
    "        # di guardare i pending_state della memory e cosa fare se ce ne sono di attivi. \n",
    "        \n",
    "        return raw  # ritorniamo la struttura di ConversationBufferMemory() piu' pulita e' con l aggiunt di pending_actions\n",
    "\n",
    "\n",
    "# Create llm\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4)\n",
    "\n",
    "\n",
    "# === MEMORY & AGENT SETUP ===\n",
    "memory = SafeConversationMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "agent = create_openai_functions_agent(\n",
    "    llm=llm,\n",
    "    tools=TOOLS,\n",
    "    prompt=prompt  # il prompt include le regole di \"Assistant Responses\" e \"Conversation Structure\"\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=TOOLS,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "# 3) - BEFORE calling any tool: produce a short HUMAN-FACING plan sentence prefixed with \"assistant_plan:\" describing what you will do next (example: \"assistant_plan: i'll check the device settings and enable Wi-Fi if needed, then search for a location\").\n",
    "\n",
    "# 4) - AFTER tool outputs or after state changes, produce a clear HUMAN-FACING summary prefixed with \"assistant_summary:\" describing the result and next step (example: \"assistant_summary: Wi-Fi was off â†’ I turned it on. Now I'll retry the search.\").\n",
    "\n",
    "# 6) - NEVER autput chain-of-thought. The plan and summary are the only \"thinking\" visible to the user.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021af571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_inputs = [\n",
    "#     \"Aggiungi una riunione per domani alle 10\",\n",
    "#     \"Cerca un podcast rilassante da ascoltare\",\n",
    "#     \"Come sta andando TSLA oggi?\"\n",
    "# ]\n",
    "\n",
    "# def search_calendar_events(title: Optional[str]=None, datetime_range_lowerbound: Optional[str]=None) -> List[str]:\n",
    "\n",
    "user_input = \"Add a meeting for tomorrow at 10\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"Is the weather good tomorrow in italy?\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"ok, the purpose is to talk with Erik\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"yes, i ll meet him at restaurant\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"Please find me a relaxing podcast to listen\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9baca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"tnx alot\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeedae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"by the way, do not modify the existing meeting\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Hoe are you by the way?\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379856be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
