{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8d3232",
   "metadata": {},
   "source": [
    "# State_dependency (With langchain and memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1b978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit self, **kwargs\n"
     ]
    }
   ],
   "source": [
    "# Unconmenti this line and replace 'key.txt' with your key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = open(\"key.txt\").read().strip() # run this code before engaging LLM model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor # the tools needed to create a sort of chain with directives\n",
    "\n",
    "# ============================\n",
    "# 1) creazione tool 'settings'\n",
    "# ============================\n",
    "\n",
    "# print('fist page') The model only call the tools and not the whole page everytime\n",
    "\n",
    "def set_system_settings(check_Key_Status:str,update_Key_Status:bool|str=None):\n",
    "    \"\"\"Check wifi and battery status. (Ricorda che il modello impara dai 'return' di queste tool e prende decisioni di conseguenza)\"\"\" \n",
    "\n",
    "    check_Key_Status = check_Key_Status.lower() # because the model can create uppercase parameters\n",
    "    print(f\"check_Key_Status here = '{check_Key_Status}' \",)\n",
    "   \n",
    "    getSettingsData = {\n",
    "      \"metadata\": {\n",
    "        \"flow\": {\n",
    "           \"initial_databases\": {\n",
    "                \"SETTING\": [\n",
    "                    {\n",
    "                        \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "                        \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "                        \"latitude\": 37.33464379999999,\n",
    "                        \"longitude\": -122.008972,\n",
    "                        \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "                        \"wifi\": False, # update_Key_Status if 'wifi' in check_Key_Status and update_Key_Status else False, # conditional inline expression\n",
    "                        \"location_service\": False,\n",
    "                        \"cellular\": False,\n",
    "                        \"low_battery_mode\": True, #update_Key_Status if 'low_battery_mode' in check_Key_Status and update_Key_Status else True,\n",
    "                        \"utc_offset_seconds\": -25200,\n",
    "                        \"locale\": \"en_US\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    \n",
    "    # get the value of the args genereted by the model\n",
    "    key_value = getSettingsData['metadata']['flow']['initial_databases']['SETTING'][0][check_Key_Status] # get the value of a system key create as argument from the model as we toldit in the systemMessage\n",
    "\n",
    "    # return F\" {check_Key_Status} status = {data_value}\"\n",
    "    if check_Key_Status and update_Key_Status is None: # Just checking the keyValue\n",
    "       print(f\"before update value = {check_Key_Status}:{key_value}\") \n",
    "  \n",
    "       return f\" Only check_Key_Status: {check_Key_Status} status = {key_value}\" # deve essere oltre lo if se vuoi che questo return avviene entro queste condizioni\n",
    "    elif check_Key_Status and isinstance(update_Key_Status,(bool,str,int)) : # update the keyValue with true false value. update_Key_Status kan be bool, str or int\n",
    "        \n",
    "        print(f\"after update value = {check_Key_Status}:{update_Key_Status}\") \n",
    "         # Same updatings in memory. We assume that check_Key_Status muste be wifi or lower_battery_mode, so we can update it correctly in memory.\n",
    "         #  We can in future have a Parameters that duble check that, but for now its good this way\n",
    "        memory.set_state_action('Key_Status',f'{check_Key_Status}', update_Key_Status)\n",
    "\n",
    "        return f\"assistant_summary: {check_Key_Status} has been updated to {update_Key_Status}. Current device state: {memory._state_actions['Key_Status']}\" # Cos√¨ il modello impara subito anche il nuovo stato, utile per evitare richiami ridondanti al tool\n",
    "        # il modello impara da questi returns, per completare la task.\n",
    "    else:\n",
    "        return f\" please enter valid arguments\" \n",
    "\n",
    "\n",
    "    \n",
    "def search_place():\n",
    "    \"\"\"Imitating a search tools that invoke goole maps. \n",
    "    (Ricorda che grazie a agent_scratchpad che utilizza il \"think step by step method\", il modello impara dai return di queste tool e prende decisioni di conseguenza)\"\"\"\n",
    "\n",
    "\n",
    "    settings_values = memory.get_state_action('Key_Status') # Get the current value of wifi and low_battery_mode settings stored in llm memory\n",
    "\n",
    "    if settings_values and settings_values['wifi'] is True and settings_values['low_battery_mode'] is False:\n",
    "\n",
    "         print(f\"search_place() wifi_status: {settings_values['wifi'] }, search_place() low_battery_mode: {settings_values['low_battery_mode']}\")\n",
    "\n",
    "         getPlaceData = [\n",
    "          [\n",
    "      {\n",
    "      \"formatted_address\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043\",\n",
    "      \"geometry\": {\n",
    "        \"location\": {\n",
    "          \"lat\": 37.4220541,\n",
    "          \"lng\": -122.0853242\n",
    "        }\n",
    "      },\n",
    "      \"name\": \"Googleplex\",\n",
    "      \"place_id\": \"ChIJj61dQgK6j4AR4GeTYWZsKWw\",\n",
    "      \"rating\": 4.2,\n",
    "      \"types\": [\n",
    "        \"point_of_interest\",\n",
    "        \"establishment\"\n",
    "        ]\n",
    "       }\n",
    "      ]\n",
    "     ]\n",
    "         return getPlaceData[0][0]['formatted_address']\n",
    "    else:\n",
    "        \n",
    "         return 'Enable wifi and disable low_battery_status to get the searched Adress' # This return is important for the model to know correctly\n",
    "         # what to do. Funge da un ulteriore direttiva, come se l avessimo scritta in systemMessage. Senza questo return, il modello non capisce correttamente che\n",
    "         # deve fare l update di wifi e low_battery_mode (anche se lo avevamo specificato nel tool set_system_setting()). Quindi il modello capisce anche \n",
    "         # dal return dei tools (e non solo dalle direttive e tool descriptions)\n",
    "         \n",
    "        # return 'wifi has to be on, and low_battery_status has to be off to get the searched Adress' # This works fine, but we defined it in the \n",
    "        # set_system_settings() tool 'description' so the model already knows about in and it doenst have to find it at runtime (like when it get that return\n",
    "        # message when ivoking this function the first time it)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2) define the Tools structure\n",
    "# =======================================\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "SETTING = [# to get the keys value to insert in the description\n",
    "  { # endta dentro il liste e prendi le key dell obj\n",
    "    \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "    \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "    \"latitude\": 37.33464379999999,\n",
    "    \"longitude\": -122.008972,\n",
    "    \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "    \"wifi\": False,\n",
    "    \"location_service\": False,\n",
    "    \"cellular\": False,\n",
    "    \"low_battery_mode\": True,\n",
    "    \"utc_offset_seconds\": -25200,\n",
    "    \"locale\": \"en_US\"\n",
    "    }\n",
    "    ]\n",
    "# print(SETTING)\n",
    "\n",
    "Settin_keys = SETTING[0].keys()  # entra dentro il list e prendi le key dell obj\n",
    "Settin_keys = list(Settin_keys) # fai diventare queste key da obj a list\n",
    "\n",
    "    \n",
    "settings_tool = StructuredTool.from_function(\n",
    "    func=set_system_settings,\n",
    "    name='set_system_settings',\n",
    "    description=\"Checks or updates system settings (wifi, low_battery_mode).\" # you can list all the keyword if needed, like the description below\n",
    "    # description=f\"\"\"Check the settings systems of a device. Requires check_Key_Status(use this keys on the list:{Settin_keys} to get a key state value), update_Key_Status(if needed, to update the 'check_Key_Status' state)\"\"\"\n",
    "    # BOTH descriptions works. The commented one is optimal when you need to check more key_state, without writing them one by one \n",
    ")\n",
    "\n",
    "search_tool = StructuredTool.from_function(\n",
    "    func=search_place,\n",
    "    name='search_place',\n",
    "    description=\"Searches for a place; Only returns valid data when wifi is on and low_battery_mode is off.\" # Con questa description, il modello verifica direttamente il setting tool prima di invocare il search_tool\n",
    "    # description='A search tool that helps find places or locations' # Con questa description invece il modello solitamente invoca search_tool al primo turn, e di conseguenza aggiusta i settings dove occorre.\n",
    ")\n",
    "\n",
    "tools = [settings_tool, search_tool]\n",
    "\n",
    "tool_descriptions = \"\\n\".join(\n",
    "    [f\"- {t.name}({', '.join(t.args_schema.__annotations__.keys())}) -> {t.description}\"\n",
    "     for t in tools]\n",
    ") # describe the tools, we add it in state_dependency_system as a source of tools data\n",
    "\n",
    "# print('tool_descriptions here =',tool_descriptions)\n",
    "\n",
    "# =======================================\n",
    "# 3) Creazione SystemMessage\n",
    "# ======================================= \n",
    "\n",
    "state_dependency_system = f\"\"\"\n",
    "You are a conversational assistant embedded in a mobile device. The user is likely interacting visually with the screen. Avoid excessive explanations and focus on highlighting only key information. Be factual, clear, and efficient. The user recently returned from international travel, is sleep-deprived, and prefers late-day activities. They have a strong preference for Asian food and often refer to their yoga instructor Lara as ‚Äúmy guide.‚Äù When suggesting options or actions, incorporate these preferences naturally without explicit reminders.\n",
    "Maintain a confident, friendly, and respectful tone. Avoid formal or overly scripted responses. The assistant should behave like a competent personal secretary: efficient, light-toned, and responsive, capable of inferring context when it is clear but cautious where ambiguity exists. Never use pleasantries like ‚Äúplease‚Äù or ‚Äúthank you,‚Äù and avoid robotic phrasing.\n",
    "\n",
    "1) - Do not assume tool parameters unless provided by the user, the prompt history, or the system context.\n",
    "\n",
    "2) - Understand the user intends and call the rigth tools.\n",
    "\n",
    "3) - BEFORE calling any tool: produce a short HUMAN-FACING plan sentence prefixed with \"assistant_plan:\" describing what you will do next (example: \"assistant_plan: i'll check the device settings and enable Wi-Fi if needed, then search for a location\").\n",
    "\n",
    "4) - AFTER tool outputs or after state changes, produce a clear HUMAN-FACING summary prefixed with \"assistant_summary:\" describing the result and next step (example: \"assistant_summary: Wi-Fi was off ‚Üí I turned it on. Now I'll retry the search.\").\n",
    "\n",
    "5) - Use ISO 8601 for any timestamp if needed.\n",
    "\n",
    "6) - NEVER autput chain-of-thought. The plan and summary are the only \"thinking\" visible to the user.\n",
    "\n",
    "7) - Whenever a user ask a question, Respond conversionally but always follow rules nr 3 and 4 around tool usage.\n",
    "\n",
    "8) - Whenever the user asks about a place or location, ALWAYS re-run the `search_place` tool to ensure up-to-date data, even if a previous result exists in memory.\n",
    "\n",
    "9) - Check the stored system settings in memory before making any decision. Only call set_system_settings if the desired setting is not already correct.\n",
    "\n",
    "10) - When the user asks about a place, location, or address (e.g., 'where is', 'find', 'locate', 'search for', etc.), you must always call the `search_place` tool, regardless of memory state or previous results.\n",
    "\n",
    "11) - Never assume a previous search result is still valid; every new user query must trigger a new search tool call.\n",
    "\n",
    "\n",
    "You have access to the following tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "When the user's request clearly matches one of these actions, \n",
    "call the appropriate tool by reasoning about its name and description..\n",
    "\"\"\"\n",
    "# When the user gives an instruction, decide if it‚Äôs:\n",
    "# - A general chat ‚Üí reply with advice, no tool use.\n",
    "\n",
    "# IMPORTANTI DIRETTIVE\n",
    "# senza questa istruzione, il modello e propenso a dare nomi degli args errati (tipa low_battery_status invece di low_battery_mode)\n",
    "# - search_place tool returns valid search data only if 'wifi' is 'on' and low_battery_mode is 'off'. \n",
    "\n",
    "# 1) - Do not assume tool parameters unless provided by the user, the prompt history, or the system context. Questa direttiva ci serve perche le args non le deve decidere il modello, ma sono quelle, a differenza della task Error_recovery dove il modello deve scegliere\n",
    "# se inserire altre args per trovare un evento(e.g. 'add date upperbound'(broadening)) oppure ometterle (e.g. omit date lowerbound (narrowing))\n",
    "\n",
    "# 8) - Whenever the user asks about a place or location, always re-run the `search_place` tool to ensure up-to-date data, even if a previous result exists in memory.\n",
    "# 10) - When the user asks about a place, location, or address (e.g., 'where is', 'find', 'locate', 'search for', etc.), you must always call the `search_place` tool, regardless of memory state or previous results.\n",
    "# 11) - Never assume a previous search result is still valid; every new user query must trigger a new search tool call.\n",
    "# Questa direttive sono importanti perche se per esempio chiediamo di nuovo 'Where is googleplex?' il modello risponde con i dati in memoria della stessa domanda\n",
    "# precedente. Mentre se usiamo questa direttiva, forziamo il modello a chiamare sempre la funzione anche se abbiamo fatto la stessa domanda identica\n",
    "# Questo serve ad avere output di dati dal tool e non dalla memoria(molto importante)\n",
    "\n",
    "# 9) - Check the stored system settings in memory before making any decision. Only call set_system_settings if the desired setting is not already correct.\n",
    "# Questa direttiva e' molto importante perche avendo i dati salvati in memoria non andremo a chiamare inutilmente una tool se non e' necessario (come descritto nella guideline)\n",
    "\n",
    "# =======================================\n",
    "# 3) Creazione del Prompt Template\n",
    "# =======================================\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(state_dependency_system), # The systeMessage\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # chat_history (niettare contenuti dinamici nel prompt.)\n",
    "    MessagesPlaceholder(variable_name=\"system_context\"), # system_context (niettare contenuti dinamici nel prompt.)\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") # 'agent_scratchpad' √à il meccanismo chiave per il reasoning step-by-step del modello.\n",
    "    # Contiene i risultati intermedi (tool call + return) della sessione corrente, cio√®:\n",
    "    # Grazie ad esso il modello apprende anche dal 'return' delle tool invocate, e non solo dalle systemMessage.\n",
    "    # Per esempio, in questo caso quando invochiamo la search tool, il return informa il modello che \"wifi deve essere on e lower_setting_state deve essere off\"\n",
    "    # e grazie a questo return informativo(che ricordiamo non deve essere troppo lungo), l'agent_scratchpad aiuta il modello a capire di agiornare i settings prima di invocare \n",
    "    # nuovamente il 'search tool'. \n",
    "    # In parole semplici: \n",
    "    # \"il modello ‚Äúvede‚Äù che la causa dell‚Äôerrore √® il Wi-Fi spento e pu√≤ decidere da solo di chiamare set_system_settings() per sistemarlo.\"\"\n",
    "])\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "# 4 Creazione della classe SafeConversationMemory(ConversationBufferMemory)\n",
    "\n",
    "class SafeConversationMemory(ConversationBufferMemory): # questa classe viene invocata ogni volta l utente interagisce con il modello, ma non viene ricreata da zero\n",
    "    # viene riutilizzata come istanza persistente, e al suo interno la chat history e gli stati vengono aggiornati.\n",
    "    # ogni volta che comunichiamo con il modello: agent_executor.invoke({\"input\": \"accendi il Wi-Fi\"}), LangChain fa questo dietro le quinte:\n",
    "\n",
    "    # 1) Chiama memory.load_memory_variables() (viene chiamato prima che l'agente costruice il prompt. \n",
    "    # Serve per elaborare i dati in memoria e servirli al modello prima che esso da' una risposta)\n",
    "    \n",
    "    # Serve per:\n",
    "    \n",
    "    # leggere la chat history precedente,\n",
    "    \n",
    "    # leggere eventuali stati (_state_actions),\n",
    "    \n",
    "    # costruire un contesto strutturato in chiave BaseMessage tipo:\n",
    "#     {\n",
    "#   \"chat_history\": [HumanMessage(...), AIMessage(...)],\n",
    "#   \"system_context\": [HumanMessage(content=\"{'wifi': False, 'low_battery_mode': True}\")]\n",
    "#    } üëâ Questo viene poi iniettato nel prompt grazie a MessagesPlaceholder (insieme al system message e all‚Äôinput utente).\n",
    "\n",
    "# 2) L‚ÄôLLM genera una risposta (eventualmente invocando tool)\n",
    "\n",
    "# 3) dopo ogni risposta del modello viene chiamata la funzione 'memory.save_context' che non vediamo qua ma avviene di sottobanco e appartiene ConversationBufferMemory.\n",
    "     # Questa funzione 'aggiorna' il HumanMessage, AIMessage e system_context, in modo che alla prossima domanda fatta al modello, questo possa \n",
    "     # prendere esempio dalle domande, risposte e system_context data del turno precedente (per esempio dopo il primo turn, memory.save_context salva gli stati aggiornati di\n",
    "     # wifi e low_battery_mode ). E' al prossimo turn 'memory.save_context' aggiorna memory.load_memory_variables() e poi il processo si ripete\n",
    "     # dal punto 1) quando lo user pone un altra domanda al modello\n",
    "\n",
    "# Entrambi i metodi che hai citato (save_context() e load_memory_variables()) appartengono alla classe base BaseMemory di LangChain, \n",
    "# e sono implementati concretamente nella sottoclasse ConversationBufferMemory (e quindi anche nella tua SafeConversationMemory)\n",
    "\n",
    "# 'load_memory_variables' viene chiamato prima che l'agente costruice il prompt (cosi prepariamo il modello a tenere conto dei dati della memory prima di leggere promp). \n",
    "# 'save_context' viene chiamato dopo che il modello ha risposto(cosi aggiorniamo per il prossimo turn, e 'load_memory_variables' prepara il modello con i nuovi dati aggiornati da 'memory.save_context' dopo che il modello ha risposto)\n",
    "    \n",
    "    \"\"\"\n",
    "    questa classe estende ConversationBufferMemory, con lo scopo di renderla piu' ‚Äúintelligente‚Äù sicura.\n",
    "    Enhanced memory that:\n",
    "    - Cleans malformed message content (list/dict -> string)\n",
    "    - Keeps track of system_settings status\n",
    "    - Add the actual status into the model through context by adding it in memory, so in the systemMessage you tell the model to verify in memory to make clever decisions \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print('Hit self, **kwargs')\n",
    "       \n",
    "        self._state_actions = {'Key_Status':{ # default status. Per far capire al modello che deve updatare questi date per accedere a serch tool\n",
    "            'wifi':False,\n",
    "            'low_battery_mode':True\n",
    "        }} # This ll be updated when we model calls set_state_action, (we can see updates when invoking memory data).\n",
    "        # Serve a memorizzare lo stato reale del sistema (non solo le parole nella chat).\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # üß† 1Ô∏è‚É£ Methods to manage pending actions\n",
    "    # ============================================================\n",
    "\n",
    "    def set_state_action(self, name: str, key:str, value:str|bool):# ( data: self, name: str, data:dict) # we call this def from the tool def create_reservation(name,data)\n",
    "        \"\"\"Store a pending action waiting for user confirmation.\"\"\"\n",
    "        # self._state_actions[name] = data\n",
    "        self._state_actions[name][key] = value # queste value e key saranno sempre le stesse 2('wifi' e 'low_batteri_mode') e per questo non si sommeranno, ma cambieranno solo le value (true or false) \n",
    "        # Vengono aggiornate uno alla volta, in ordine del modello quando invoka l'update delle key value nella tool set_system_settings\n",
    "\n",
    "    def get_state_action(self, name: str): # controls if there are any pending actions when the model calls add_to_calender(name) Tool\n",
    "        \"\"\"Retrieve a specific pending action.\"\"\"\n",
    "        return self._state_actions.get(name) # invocando questa funzione avremo l attuale status delle key 'wifi' e 'low_batteri_mode'\n",
    "\n",
    "    # ============================================================\n",
    "    # üßπ 2Ô∏è‚É£ Override: cleaning + injecting context for the model\n",
    "    # ============================================================\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Return cleaned chat history + pending actions as context.\"\"\"\n",
    "        raw = super().load_memory_variables(inputs) # il nome del method della classe ConversationBufferMemory, che qui estendiamo e modifichiamo per fare \n",
    "        # pulizie e aggiungere system_context in formato BaseMessage. (deve avere questo nome load_memory_variables)\n",
    "\n",
    "        print('hit load_memory_variables and inputs :',inputs) # {'input': 'where is googleplex?'}\n",
    "        # ---- Clean malformed messages (lists/dicts converted to strings)\n",
    "        if \"chat_history\" in raw:  # chat_history e' la key che abbiamo dato al nostro memoryKey quando abbiamo definito 'memory' e nel prompt\n",
    "            cleaned = []\n",
    "            print('hit chat_history')\n",
    "            for msg in raw[\"chat_history\"]:\n",
    "                if isinstance(msg, BaseMessage): # BaseMessage(HumanMessage(content:'user_input') and AIMessage(content:'ai response')) is the rigth type for our model. Its the type the model require to work without errors.\n",
    "                    # It is like that:\n",
    "                    # {'chat_history': [HumanMessage(content='I need a reservation', additional_kwargs={}, response_metadata={}), AIMessage(content='Could you please clarify what type of reservation you need? For example, is it for a restaurant, hotel, or something else? Additionally, please provide any specific details like the date, time, and number of people if applicable.', additional_kwargs={}, response_metadata={})}\n",
    "                    # so basically our memory stored in ConversationBufferMemory is stored in this way showed in HumanMessage and AIMessage\n",
    "                    msg.content = str(msg.content) # turn (list, dict) content in str type to not throw errors\n",
    "                    cleaned.append(msg)\n",
    "                elif isinstance(msg, str): # if there are some 'fully string type' value that can cause errors(like not structured baseMessage data), instead of BaseMessage structure in our\n",
    "                      # ConversationBufferMemory chat_history , then turn this string  to HumanMessage(). This 'string' bug can hapend when the model calls a tool, creating a \n",
    "                      # 'string with the name of the tool invoked', that the Model doesnt support and cannot parse.\n",
    "                      #(HumanMessage is part of baseMessage for related to the user_input. there is the exemple right up in 'if isinstance(msg, BaseMessage)'). \n",
    "                     cleaned.append(HumanMessage(content=msg)) \n",
    "            raw[\"chat_history\"] = cleaned # puliamo l object chat_history\n",
    "            print('check raw ->', raw)\n",
    "\n",
    "        # ---- Add pending actions as extra conversational context. Questo previene chiamate al tool \"reservation\" quando invokiamo il tool add_to_calender\n",
    "        # inseriamo in memoria che c'e un pending tool ogni volta che il tool \"reservation\" viene invocato, e nel systemMessage diciamo specificamente\n",
    "        # al modello di non invocare nuovamente il tool \"reservation\" quando decidiamo di mettere questo \"reservatio event\" come evento nel tool \"calender\"\n",
    "        # infine ogni volta che il modello inserisce l'evento sul tool calender, il pendig state ritorna vuoto, cosicche' il processo si ripete e \n",
    "        # non ci ripeta lo stesso problema\n",
    "        if self._state_actions: # if _state_actions not empty, cioe' se ci sono dei pending_state \n",
    "            context_str = (\n",
    "                f\"(State actions currently stored: {self._state_actions})\" # questo va a' finire nella memory nel formato humaMessage come bediamo sotto\n",
    "            )\n",
    "            print('check state_actions ->', context_str)\n",
    "        else:\n",
    "            context_str = \"(No state actions stored.)\"\n",
    "        \n",
    "        # aggiungiamo pending_actions in formato HumanMessage\n",
    "        raw[\"system_context\"] = [HumanMessage(content=context_str)] # aggiungiamo un messaggio extra che contiene i dati di ogni turn delle values dei settngs\n",
    "        # sotto il prefisso 'system_context'(inventato da me) che poi inseriamo nel prompt/AIMessagePlaceHolder, cosi ad ogni turno il modello tiene \n",
    "        # conto dei dati ogiornati o meno di settings del turno precedente\n",
    "        # Lo stesso vale per chat_history\n",
    "        # trasformiamo il pending action in uno HumanMessage in modo che il formato sia\n",
    "        # corretto da inserire nella memory e da essere compatimile con il formato del modello.\n",
    "        # Ora  Il modello ‚Äúvede‚Äù il pending_action nel contesto e pu√≤ comportarsi di conseguenza (nei punti 5 e 6 del systemMessage diciamo al\n",
    "        # di guardare i pending_state della memory e cosa fare se ce ne sono di attivi. \n",
    "        \n",
    "        return raw  # ritorniamo la struttura di ConversationBufferMemory() piu' pulita e' con l aggiunt di pending_actions\n",
    "\n",
    "\n",
    "memory = SafeConversationMemory( # istanza della classe SafeConversationMemory. Qui passiamo anche questi parametri definiti qua dentro alla classe\n",
    "    memory_key=\"chat_history\", # crea il BaseMessage della cronologia tra user e model(HumanMessage,AiMessage)\n",
    "    return_messages=True,\n",
    "    input_key=\"input\"  # üëà forza un solo input\n",
    ")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# ‚úÖ  Creazione modello\n",
    "# =======================================\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# ‚úÖ  Creazione agente\n",
    "# =======================================\n",
    "\n",
    "agent = create_openai_functions_agent( # creiamo il nostro agente strutturato\n",
    "    llm=llm,\n",
    "    tools = tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools= tools, \n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65df2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit load_memory_variables and inputs : {'input': 'where is googleplex?'}\n",
      "hit chat_history\n",
      "check raw -> {'chat_history': []}\n",
      "check state_actions -> (State actions currently stored: {'Key_Status': {'wifi': False, 'low_battery_mode': True}})\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `set_system_settings` with `{'check_Key_Status': 'wifi', 'update_Key_Status': True}`\n",
      "\n",
      "\n",
      "\u001b[0mcheck_Key_Status here = 'wifi' \n",
      "after update value = wifi:True\n",
      "\u001b[36;1m\u001b[1;3massistant_summary: wifi has been updated to True. Current device state: {'wifi': True, 'low_battery_mode': True}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_place` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mEnable wifi and disable low_battery_status to get the searched Adress\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `set_system_settings` with `{'check_Key_Status': 'low_battery_mode', 'update_Key_Status': False}`\n",
      "\n",
      "\n",
      "\u001b[0mcheck_Key_Status here = 'low_battery_mode' \n",
      "after update value = low_battery_mode:False\n",
      "\u001b[36;1m\u001b[1;3massistant_summary: low_battery_mode has been updated to False. Current device state: {'wifi': True, 'low_battery_mode': False}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_place` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0msearch_place() wifi_status: True, search_place() low_battery_mode: False\n",
      "\u001b[33;1m\u001b[1;3m1600 Amphitheatre Pkwy, Mountain View, CA 94043\u001b[0m\u001b[32;1m\u001b[1;3massistant_summary: The Googleplex is located at 1600 Amphitheatre Pkwy, Mountain View, CA 94043.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "assistant_summary: The Googleplex is located at 1600 Amphitheatre Pkwy, Mountain View, CA 94043.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"where is googleplex?\" \n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bda08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit load_memory_variables and inputs : {'input': 'tnx chat'}\n",
      "hit chat_history\n",
      "check raw -> {'chat_history': [HumanMessage(content='where is googleplex?', additional_kwargs={}, response_metadata={}), AIMessage(content='assistant_summary: The Googleplex is located at 1600 Amphitheatre Pkwy, Mountain View, CA 94043.', additional_kwargs={}, response_metadata={})]}\n",
      "check state_actions -> (State actions currently stored: {'Key_Status': {'wifi': True, 'low_battery_mode': False}})\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3massistant_summary: No action needed. If you need anything else, just let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "assistant_summary: No action needed. If you need anything else, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# No useless function call when it is a general question or a simple 'tnx'\n",
    "user_input = \"tnx chat\" #\"Aggiungi una sessione di stretching alle 7:30.\"\n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THOSE INPUT TO SEE HOW THE MODEL REACT\n",
    "\n",
    "# user_input = \"Ok, where is Googleplex?\" # the model always invoke search_place tool to guarantee up-to-date data\n",
    "# response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "# print(response[\"output\"]) # output where you get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondo turn: il modello perche i parametri wifi e lower_battery_status sono salvati corretti in in memoria e quindi decide che non serve \n",
    "# richiamare la search_place(), e risponde ‚ÄúIl Googleplex si trova a Mountain View‚Ä¶‚Äù basandosi solo sulla memoria.\n",
    "\n",
    "# USE THOSE INPUT TO SEE HOW THE MODEL REACT\n",
    "# user_input = \"where is California?\" \n",
    "# response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "# print(response[\"output\"]) # output where you get the answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ae87c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5545628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17484faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
