{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-openai\n",
    "# !pip install langchain\n",
    "\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = open(\"key.txt\").read().strip() # run this code before engaging LLM model\n",
    "\n",
    "\n",
    "# state_dependency_system = f\"\"\"\n",
    "# You are a conversational assistant embedded in a mobile device. The user is likely interacting visually with the screen. Avoid excessive explanations and focus on highlighting only key information. Be factual, clear, and efficient. The user recently returned from international travel, is sleep-deprived, and prefers late-day activities. They have a strong preference for Asian food and often refer to their yoga instructor Lara as ‚Äúmy guide.‚Äù When suggesting options or actions, incorporate these preferences naturally without explicit reminders.\n",
    "\n",
    "# Maintain a confident, friendly, and respectful tone. Avoid formal or overly scripted responses. The assistant should behave like a competent personal secretary: efficient, light-toned, and responsive, capable of inferring context when it is clear but cautious where ambiguity exists. Never use pleasantries like ‚Äúplease‚Äù or ‚Äúthank you,‚Äù and avoid robotic phrasing.\n",
    "\n",
    "#   - Before changing the setting first fetch the settings and check if the change is actually needed or not.\n",
    "   \n",
    "#   - If a setting was previously validated and remains unchanged, you do not need to re-check it.\n",
    " \n",
    "#   - You can invoke 'settings_tool' and 'search_tool' and enabling data inside those tools without asking user for comfirmation.\n",
    "   \n",
    "#   - Even if some setting is preventing other setting to run then change that setting as well without asking user for confirmation.\n",
    "   \n",
    "#   - Always ask which calendar to use, unless it is implicitly clear from context.\n",
    "   \n",
    "#   - Before creating any event, search for overlapping events in the specified calendar and notify the user of any conflicts.\n",
    "   \n",
    "#   - When events contain a location, ensure all components (place_id, lat, lng, and formatted_address) are included.\n",
    "   \n",
    "#   - For recurring events, use instance_id to modify one occurrence, and parent_id to affect all. Confirm with the user before modifying or deleting.\n",
    "  \n",
    "#   - Always summarize every event field upon creation or update: title, time, calendar, location, recurrence, and any notes.\n",
    "   \n",
    "#   - You cannot directly book or cancel with external vendors. Instead:\n",
    "\n",
    "#     - Offer to block the time in the user's calendar.\n",
    "#     - Share contact information so the user can handle it themselves.\n",
    "#     - Make it clear that no external booking was made unless the user completes the action independently.\n",
    "\n",
    "#   - Do not assume tool parameters unless provided by the user, the prompt history, or the system context.\n",
    "\n",
    "#   - Avoid follow-up prompts that simply repeat or narrow the previous prompt without new logic.\n",
    "   \n",
    "#   - Keep conversations logically flowing. No abrupt context switches or restart of new topics midway.\n",
    "  \n",
    "#   - Replace specific names with general terms where possible (\"this park\", \"that place\", \"this event\").\n",
    "   \n",
    "#   - Ask clarifying questions when user input is ambiguous.\n",
    "  \n",
    "#   - Avoid referencing system internals or exposing the concept of tool-based architecture.\n",
    "   \n",
    "#   - Use ISO 8601 format with timezone offsets for all timestamps.\n",
    "   \n",
    "#   - Avoid hallucinated facts. Only respond with tool output or confirmed context information.\n",
    "   \n",
    "#   - Do not make assumptions about the user's calendar, location, or preferences unless explicitly confirmed or clearly inferred.\n",
    "\n",
    "#   - Treat every interaction as if you are helping a real-world user in a production-grade assistant.\n",
    "\n",
    "# You have access to the following tools:\n",
    "# {tool_descriptions}.\n",
    "# Call the appropriate tool by reasoning about its name and description. \n",
    "#    - Beside 'settings_tool' and 'search_tool', always ask confermations before invoking a tool.\n",
    "# \"\"\"\n",
    "# e meglio ritoccare il systemMessage, perche non serve tutto questo materiale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_place():\n",
    "#     \"\"\"Imitating a search tools that invoke goole maps\"\"\"\n",
    "#     getPlaceData = [\n",
    "#   [\n",
    "#     {\n",
    "#       \"formatted_address\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043\",\n",
    "#       \"geometry\": {\n",
    "#         \"location\": {\n",
    "#           \"lat\": 37.4220541,\n",
    "#           \"lng\": -122.0853242\n",
    "#         }\n",
    "#       },\n",
    "#       \"name\": \"Googleplex\",\n",
    "#       \"place_id\": \"ChIJj61dQgK6j4AR4GeTYWZsKWw\",\n",
    "#       \"rating\": 4.2,\n",
    "#       \"types\": [\n",
    "#         \"point_of_interest\",\n",
    "#         \"establishment\"\n",
    "#       ]\n",
    "#     }\n",
    "#   ]\n",
    "#   ]\n",
    "#     return getPlaceData[0][0]['formatted_address']\n",
    "\n",
    "# print(search_place())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconmenti this line and replace 'key.txt' with your key\n",
    "# import os \n",
    "# os.environ[\"OPENAI_API_KEY\"] = open(\"key.txt\").read().strip() # run this code before engaging LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ada79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor # the tools needed to create a sort of chain with directives\n",
    "\n",
    "# ============================\n",
    "# 1) creazione tool 'settings'\n",
    "# ============================\n",
    "\n",
    "# global wifi \n",
    "# global low_battery_status\n",
    "\n",
    "wifi_status = False\n",
    "low_battery_status = True\n",
    "\n",
    "print('fist page')\n",
    "\n",
    "def set_system_settings(check_Key_Status:str,update_Key_Status:bool|str=None):\n",
    "    \"\"\"Check wifi and battery status\"\"\" \n",
    "\n",
    "    check_Key_Status = check_Key_Status.lower() # the model can pass uppercase as parameters\n",
    "    print(f\"check_Key_Status here = '{check_Key_Status}' \",)\n",
    "    # print(f\"update_Key_Value here = '{update_Key_Status}' \",)\n",
    "\n",
    "    getSettingsData = {\n",
    "      \"metadata\": {\n",
    "        \"flow\": {\n",
    "           \"initial_databases\": {\n",
    "                \"SETTING\": [\n",
    "                    {\n",
    "                        \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "                        \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "                        \"latitude\": 37.33464379999999,\n",
    "                        \"longitude\": -122.008972,\n",
    "                        \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "                        \"wifi\": False, # update_Key_Status if 'wifi' in check_Key_Status and update_Key_Status else False, # conditional inline expression\n",
    "                        \"location_service\": False,\n",
    "                        \"cellular\": False,\n",
    "                        \"low_battery_mode\": True, #update_Key_Status if 'low_battery_mode' in check_Key_Status and update_Key_Status else True,\n",
    "                        \"utc_offset_seconds\": -25200,\n",
    "                        \"locale\": \"en_US\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    \n",
    "    # get the value of the args genereted by the model\n",
    "    key_value = getSettingsData['metadata']['flow']['initial_databases']['SETTING'][0][check_Key_Status] # get the value of a system key create as argument from the model as we toldit in the systemMessage\n",
    "    \n",
    "    \n",
    "    if check_Key_Status and update_Key_Status is None: # Just checking the keyValue\n",
    "       print(f\"before update value = {check_Key_Status}:{key_value}\") \n",
    "\n",
    "       return f\" Only check_Key_Status: {check_Key_Status} status = {key_value}\" # deve essere oltre lo if se vuoi che questo return avviene entro queste condizioni\n",
    "    elif check_Key_Status and isinstance(update_Key_Status,(bool,str,int)) : # update the keyValue with true false value. update_Key_Status kan be bool, str or int\n",
    "        print(f\"after update value = {check_Key_Status}:{update_Key_Status}\") \n",
    "\n",
    "        if check_Key_Status == 'wifi':\n",
    "            global wifi_status\n",
    "            print(f\"check_Key_Status = wifi: {check_Key_Status}\")\n",
    "            wifi_status = update_Key_Status\n",
    "        elif check_Key_Status == 'low_battery_mode':\n",
    "            global low_battery_status\n",
    "            low_battery_status = update_Key_Status\n",
    "\n",
    "    #     memory.set_state_action(\"Settings_Status\",{ # tels the module the status of a searched key setting\n",
    "    #         # 'test': {update_Key_Status}\n",
    "    #           f\"{check_Key_Status}\":update_Key_Status # update the status of the setting key and save it in memory, so we can better train the model by consult  memory before make a decision\n",
    "    #    })\n",
    "        return f\" check_Key_Status and update_Key_Status: {check_Key_Status} status = {update_Key_Status}\"\n",
    "    else:\n",
    "        return f\" please enter valid arguments\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def search_place():\n",
    "    \"\"\"Imitating a search tools that invoke goole maps\"\"\"\n",
    "\n",
    "    # check_setting_status = memory.get_state_action('Settings_Status')\n",
    "\n",
    "    if wifi_status is True and low_battery_status is False:\n",
    "         print(f\"search_place() wifi_status: {wifi_status}, search_place() low_battery_status: {low_battery_status}\")\n",
    "         getPlaceData = [\n",
    "         [\n",
    "      {\n",
    "      \"formatted_address\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043\",\n",
    "      \"geometry\": {\n",
    "        \"location\": {\n",
    "          \"lat\": 37.4220541,\n",
    "          \"lng\": -122.0853242\n",
    "        }\n",
    "      },\n",
    "      \"name\": \"Googleplex\",\n",
    "      \"place_id\": \"ChIJj61dQgK6j4AR4GeTYWZsKWw\",\n",
    "      \"rating\": 4.2,\n",
    "      \"types\": [\n",
    "        \"point_of_interest\",\n",
    "        \"establishment\"\n",
    "        ]\n",
    "       }\n",
    "      ]\n",
    "     ]\n",
    "         return getPlaceData[0][0]['formatted_address']\n",
    "    else:\n",
    "         return 'wifi has to turn on, and low_battery_status has to turn off'\n",
    "\n",
    "\n",
    "# print(search_place())\n",
    "# =======================================\n",
    "# 2) define the Tools structure\n",
    "# =======================================\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# SETTING = [# to get the keys value to insert in the description\n",
    "#   { # endta dentro il liste e prendi le key dell obj\n",
    "#     \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "#     \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "#     \"latitude\": 37.33464379999999,\n",
    "#     \"longitude\": -122.008972,\n",
    "#     \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "#     \"wifi\": False,\n",
    "#     \"location_service\": False,\n",
    "#     \"cellular\": False,\n",
    "#     \"low_battery_mode\": True,\n",
    "#     \"utc_offset_seconds\": -25200,\n",
    "#     \"locale\": \"en_US\"\n",
    "#     }\n",
    "#     ]\n",
    "# # print(SETTING)\n",
    "\n",
    "# Settin_keys = SETTING[0].keys()  # entra dentro il list e prendi le key dell obj\n",
    "# Settin_keys = list(Settin_keys) # fai diventare queste key da obj a list\n",
    "\n",
    "    \n",
    "settings_tool = StructuredTool.from_function(\n",
    "    func=set_system_settings,\n",
    "    name='set_system_settings',\n",
    "    description=f\"\"\"Check the settings systems of a device. Requires check_Key_Status(to get a key state), update_Key_Status( if needed, to update the 'check_Key_Status' state)\"\"\"\n",
    "     \n",
    "    # Request argument {Settin_keys}. Use Those keys as argument to check or change values when necessary\"\n",
    ")\n",
    "\n",
    "search_tool = StructuredTool.from_function(\n",
    "    func=search_place,\n",
    "    name='search_place',\n",
    "    description='a search tool that helps find places or locations'\n",
    ")\n",
    "\n",
    "tools = [settings_tool, search_tool]\n",
    "\n",
    "tool_descriptions = \"\\n\".join(\n",
    "    [f\"- {t.name}({', '.join(t.args_schema.__annotations__.keys())}) -> {t.description}\"\n",
    "     for t in tools]\n",
    ")\n",
    "\n",
    "# print('tool_descriptions here =',tool_descriptions)\n",
    "\n",
    "# =======================================\n",
    "# 3) Creazione SystemMessage\n",
    "# ======================================= \n",
    "\n",
    "state_dependency_system = f\"\"\"\n",
    "You are a conversational assistant embedded in a mobile device. The user is likely interacting visually with the screen. Avoid excessive explanations and focus on highlighting only key information. Be factual, clear, and efficient. The user recently returned from international travel, is sleep-deprived, and prefers late-day activities. They have a strong preference for Asian food and often refer to their yoga instructor Lara as ‚Äúmy guide.‚Äù When suggesting options or actions, incorporate these preferences naturally without explicit reminders.\n",
    "Maintain a confident, friendly, and respectful tone. Avoid formal or overly scripted responses. The assistant should behave like a competent personal secretary: efficient, light-toned, and responsive, capable of inferring context when it is clear but cautious where ambiguity exists. Never use pleasantries like ‚Äúplease‚Äù or ‚Äúthank you,‚Äù and avoid robotic phrasing.\n",
    "\n",
    "- If 'wifi' is 'off' and 'low_battery_mode' is 'on', turn 'wifi' 'on' and 'low_battery_mode' 'off', by invoking the right tool.\n",
    "\n",
    "- Do not assume tool parameters unless provided by the user, the prompt history, or the system context.\n",
    "\n",
    "- Understand the user intends and call the rigth tools.\n",
    "\n",
    "- When needed, update the system settings until the response is matching the user request.\n",
    "\n",
    "- search_place tool returns valid search data only if 'wifi' is 'on' and low_battery_mode is 'off'.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "When the user's request clearly matches one of these actions, \n",
    "call the appropriate tool by reasoning about its name and description..\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# good ones. con questi due istruzioni riusciamo quasi a completare la task\n",
    "# - If 'wifi' is 'off' and 'low_battery_mode' is 'on', turn 'wifi' 'on' and 'low_battery_mode' 'off', by invoking the right tool.\n",
    "# - search_place tool returns valid search data only if 'wifi' is 'on' and low_battery_mode is 'off'.\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3) Creazione del Prompt Template\n",
    "# =======================================\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(state_dependency_system),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "# 4 Creazione della classe SafeConversationMemory(ConversationBufferMemory)\n",
    "\n",
    "class SafeConversationMemory(ConversationBufferMemory):\n",
    "    \"\"\"\n",
    "    Enhanced memory that:\n",
    "    - Cleans malformed message content (list/dict -> string)\n",
    "    - Keeps track of pending actions between user confirmations\n",
    "    - Exposes pending actions to the model through context by adding it in memory, so in the systemMessage you tell the model to verify in memory\n",
    "    - if there are pending actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self._state_actions = {}  # store actions awaiting confirmation\n",
    "        # self._state_actions = {'Settings_Status': {'check_Key_Status': 'wifii', 'updateKeyStatus': 'True'}}\n",
    "\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # üß† 1Ô∏è‚É£ Methods to manage pending actions\n",
    "    # ============================================================\n",
    "\n",
    "    def set_state_action(self, name: str, data: dict): # we call this def from the tool def create_reservation(name,data)\n",
    "        \"\"\"Store a pending action waiting for user confirmation.\"\"\"\n",
    "        self._state_actions[name] = data\n",
    "\n",
    "    def get_state_action(self, name: str): # controls if there are any pending actions when the model calls add_to_calender(name) Tool\n",
    "        \"\"\"Retrieve a specific pending action.\"\"\"\n",
    "        return self._state_actions.get(name)\n",
    "\n",
    "    # def clear_pending_action(self, name: str = None):\n",
    "    #     \"\"\"Clear one or all pending actions.\"\"\"\n",
    "    #     if name:\n",
    "    #         self._state_actions.pop(name, None)\n",
    "    #     else:\n",
    "    #         self._state_actions.clear()\n",
    "\n",
    "    # ============================================================\n",
    "    # üßπ 2Ô∏è‚É£ Override: cleaning + injecting context for the model\n",
    "    # ============================================================\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Return cleaned chat history + pending actions as context.\"\"\"\n",
    "        raw = super().load_memory_variables(inputs)\n",
    "\n",
    "        # ---- Clean malformed messages (lists/dicts converted to strings)\n",
    "        if \"chat_history\" in raw:  # chat_history e' la key che abbiamo dato al nostro quando abbiamo definito 'memory'\n",
    "            cleaned = []\n",
    "            for msg in raw[\"chat_history\"]:\n",
    "                if isinstance(msg, BaseMessage): # BaseMessage(HumanMessage(content:'user_input') and AIMessage(content:'ai response')) is the rigth type for our model. Its the type the model require to work without errors.\n",
    "                    # It is like that:\n",
    "                    # {'chat_history': [HumanMessage(content='I need a reservation', additional_kwargs={}, response_metadata={}), AIMessage(content='Could you please clarify what type of reservation you need? For example, is it for a restaurant, hotel, or something else? Additionally, please provide any specific details like the date, time, and number of people if applicable.', additional_kwargs={}, response_metadata={})}\n",
    "                    # so basically our memory stored in ConversationBufferMemory is stored in this way showed in HumanMessage and AIMessage\n",
    "                    msg.content = str(msg.content) # turn (list, dict) content in str type to not throw errors\n",
    "                    cleaned.append(msg)\n",
    "                elif isinstance(msg, str): # if there are some 'fully string type' value(like not structured baseMessage data), instead of BaseMessage structure in our\n",
    "                      # ConversationBufferMemory chat_history , then turn this string  to HumanMessage(). This 'string' bug can hapend when the model calls a tool, creating a \n",
    "                      # 'string with the name of the tool invoked', that the Model doesnt support and cannot parse.\n",
    "                      #(HumanMessage is part of baseMessage for related to the user_input. there is the exemple right up in 'if isinstance(msg, BaseMessage)'). \n",
    "                     cleaned.append(HumanMessage(content=msg)) \n",
    "            raw[\"chat_history\"] = cleaned # puliamo l object chat_history\n",
    "\n",
    "        # ---- Add pending actions as extra conversational context. Questo previene chiamate al tool \"reservation\" quando invokiamo il tool add_to_calender\n",
    "        # inseriamo in memoria che c'e un pending tool ogni volta che il tool \"reservation\" viene invocato, e nel systemMessage diciamo specificamente\n",
    "        # al modello di non invocare nuovamente il tool \"reservation\" quando decidiamo di mettere questo \"reservatio event\" come evento nel tool \"calender\"\n",
    "        # infine ogni volta che il modello inserisce l'evento sul tool calender, il pendig state ritorna vuoto, cosicche' il processo si ripete e \n",
    "        # non ci ripeta lo stesso problema\n",
    "        if self._state_actions: # if _state_actions not empty, cioe' se ci sono dei pending_state \n",
    "            context_str = (\n",
    "                f\"(State actions currently stored: {self._state_actions})\" # questo va a' finire nella memory nel formato humaMessage come bediamo sotto\n",
    "            )\n",
    "        else:\n",
    "            context_str = \"(No state actions stored.)\"\n",
    "        \n",
    "        # aggiungiamo pending_actions in formato HumanMessage\n",
    "        raw[\"pending_actions\"] = [HumanMessage(content=context_str)] # trasformiamo il pending action in uno HumanMessage in modo che il formato sia\n",
    "        # corretto da inserre nella memory e da essere compatimile con il formato del modello.\n",
    "        # Ora  Il modello ‚Äúvede‚Äù il pending_action nel contesto e pu√≤ comportarsi di conseguenza (nei punti 5 e 6 del systemMessage diciamo al\n",
    "        # di guardare i pending_state della memory e cosa fare se ce ne sono di attivi. \n",
    "        \n",
    "        return raw  # ritorniamo la struttura di ConversationBufferMemory() piu' pulita e' con l aggiunt di pending_actions\n",
    "\n",
    "\n",
    "\n",
    "# Struttura della memory da passare agli agenti\n",
    "# memory = ConversationBufferMemory( # istanza della classe SafeConversationMemory. Qui passiamo anche questi parametri definiti qua dentro alla classe\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=True,\n",
    "#     input_key=\"input\"  # üëà forza un solo input\n",
    "# )\n",
    "\n",
    "memory = SafeConversationMemory( # istanza della classe SafeConversationMemory. Qui passiamo anche questi parametri definiti qua dentro alla classe\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"input\"  # üëà forza un solo input\n",
    ")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# ‚úÖ  Creazione modello\n",
    "# =======================================\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# ‚úÖ  Creazione agente\n",
    "# =======================================\n",
    "\n",
    "\n",
    "agent = create_openai_functions_agent( # creiamo il nostro agente strutturato\n",
    "    llm=llm,\n",
    "    tools = tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools= tools, \n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Where is googleplex?\"\n",
    "response = agent_executor.invoke({'input':user_input})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1faab86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d8d3232",
   "metadata": {},
   "source": [
    "# State_dependency (Improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconmenti this line and replace 'key.txt' with your key\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = open(\"key.txt\").read().strip() # run this code before engaging LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconmenti this line and replace 'key.txt' with your key\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = open(\"key.txt\").read().strip() # run this code before engaging LLM model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor # the tools needed to create a sort of chain with directives\n",
    "\n",
    "# ============================\n",
    "# 1) creazione tool 'settings'\n",
    "# ============================\n",
    "\n",
    "# print('fist page') The model only call the tools and not the whole page everytime\n",
    "\n",
    "def set_system_settings(check_Key_Status:str,update_Key_Status:bool|str=None):\n",
    "    \"\"\"Check wifi and battery status. (Ricorda che il modello impara dai 'return' di queste tool e prende decisioni di conseguenza)\"\"\" \n",
    "\n",
    "    check_Key_Status = check_Key_Status.lower() # because the model can create uppercase parameters\n",
    "    print(f\"check_Key_Status here = '{check_Key_Status}' \",)\n",
    "   \n",
    "    getSettingsData = {\n",
    "      \"metadata\": {\n",
    "        \"flow\": {\n",
    "           \"initial_databases\": {\n",
    "                \"SETTING\": [\n",
    "                    {\n",
    "                        \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "                        \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "                        \"latitude\": 37.33464379999999,\n",
    "                        \"longitude\": -122.008972,\n",
    "                        \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "                        \"wifi\": False, # update_Key_Status if 'wifi' in check_Key_Status and update_Key_Status else False, # conditional inline expression\n",
    "                        \"location_service\": False,\n",
    "                        \"cellular\": False,\n",
    "                        \"low_battery_mode\": True, #update_Key_Status if 'low_battery_mode' in check_Key_Status and update_Key_Status else True,\n",
    "                        \"utc_offset_seconds\": -25200,\n",
    "                        \"locale\": \"en_US\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    \n",
    "    # get the value of the args genereted by the model\n",
    "    key_value = getSettingsData['metadata']['flow']['initial_databases']['SETTING'][0][check_Key_Status] # get the value of a system key create as argument from the model as we toldit in the systemMessage\n",
    "\n",
    "    # return F\" {check_Key_Status} status = {data_value}\"\n",
    "    if check_Key_Status and update_Key_Status is None: # Just checking the keyValue\n",
    "       print(f\"before update value = {check_Key_Status}:{key_value}\") \n",
    "  \n",
    "       return f\" Only check_Key_Status: {check_Key_Status} status = {key_value}\" # deve essere oltre lo if se vuoi che questo return avviene entro queste condizioni\n",
    "    elif check_Key_Status and isinstance(update_Key_Status,(bool,str,int)) : # update the keyValue with true false value. update_Key_Status kan be bool, str or int\n",
    "        \n",
    "        print(f\"after update value = {check_Key_Status}:{update_Key_Status}\") \n",
    "         # Same updatings in memory. We assume that check_Key_Status muste be wifi or lower_battery_mode, so we can update it correctly in memory.\n",
    "         #  We can in future have a Parameters that duble check that, but for now its good this way\n",
    "        memory.set_state_action('Key_Status',f'{check_Key_Status}', update_Key_Status)\n",
    "\n",
    "        return f\"assistant_summary: {check_Key_Status} has been updated to {update_Key_Status}. Current device state: {memory._state_actions['Key_Status']}\" # Cos√¨ il modello impara subito anche il nuovo stato, utile per evitare richiami ridondanti al tool\n",
    "        # il modello impara da questi returns, per completare la task.\n",
    "    else:\n",
    "        return f\" please enter valid arguments\" \n",
    "\n",
    "\n",
    "    \n",
    "def search_place():\n",
    "    \"\"\"Imitating a search tools that invoke goole maps. \n",
    "    (Ricorda che grazie a agent_scratchpad che utilizza il \"think step by step method\", il modello impara dai return di queste tool e prende decisioni di conseguenza)\"\"\"\n",
    "\n",
    "\n",
    "    settings_values = memory.get_state_action('Key_Status') # Get the current value of wifi and low_battery_mode settings stored in llm memory\n",
    "\n",
    "    if settings_values and settings_values['wifi'] is True and settings_values['low_battery_mode'] is False:\n",
    "\n",
    "         print(f\"search_place() wifi_status: {settings_values['wifi'] }, search_place() low_battery_mode: {settings_values['low_battery_mode']}\")\n",
    "\n",
    "         getPlaceData = [\n",
    "          [\n",
    "      {\n",
    "      \"formatted_address\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043\",\n",
    "      \"geometry\": {\n",
    "        \"location\": {\n",
    "          \"lat\": 37.4220541,\n",
    "          \"lng\": -122.0853242\n",
    "        }\n",
    "      },\n",
    "      \"name\": \"Googleplex\",\n",
    "      \"place_id\": \"ChIJj61dQgK6j4AR4GeTYWZsKWw\",\n",
    "      \"rating\": 4.2,\n",
    "      \"types\": [\n",
    "        \"point_of_interest\",\n",
    "        \"establishment\"\n",
    "        ]\n",
    "       }\n",
    "      ]\n",
    "     ]\n",
    "         return getPlaceData[0][0]['formatted_address']\n",
    "    else:\n",
    "        \n",
    "         return 'Enable wifi and disable low_battery_status to get the searched Adress' # This return is important for the model to know correctly\n",
    "         # what to do. Funge da un ulteriore direttiva, come se l avessimo scritta in systemMessage. Senza questo return, il modello non capisce correttamente che\n",
    "         # deve fare l update di wifi e low_battery_mode (anche se lo avevamo specificato nel tool set_system_setting()). Quindi il modello capisce anche \n",
    "         # dal return dei tools (e non solo dalle direttive e tool descriptions)\n",
    "         \n",
    "        # return 'wifi has to be on, and low_battery_status has to be off to get the searched Adress' # This works fine, but we defined it in the \n",
    "        # set_system_settings() tool 'description' so the model already knows about in and it doenst have to find it at runtime (like when it get that return\n",
    "        # message when ivoking this function the first time it)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2) define the Tools structure\n",
    "# =======================================\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "SETTING = [# to get the keys value to insert in the description\n",
    "  { # endta dentro il liste e prendi le key dell obj\n",
    "    \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "    \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "    \"latitude\": 37.33464379999999,\n",
    "    \"longitude\": -122.008972,\n",
    "    \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "    \"wifi\": False,\n",
    "    \"location_service\": False,\n",
    "    \"cellular\": False,\n",
    "    \"low_battery_mode\": True,\n",
    "    \"utc_offset_seconds\": -25200,\n",
    "    \"locale\": \"en_US\"\n",
    "    }\n",
    "    ]\n",
    "# print(SETTING)\n",
    "\n",
    "Settin_keys = SETTING[0].keys()  # entra dentro il list e prendi le key dell obj\n",
    "Settin_keys = list(Settin_keys) # fai diventare queste key da obj a list\n",
    "\n",
    "    \n",
    "settings_tool = StructuredTool.from_function(\n",
    "    func=set_system_settings,\n",
    "    name='set_system_settings',\n",
    "    description=\"Checks or updates system settings (wifi, low_battery_mode).\" # you can list all the keyword if needed, like the description below\n",
    "    # description=f\"\"\"Check the settings systems of a device. Requires check_Key_Status(use this keys on the list:{Settin_keys} to get a key state value), update_Key_Status(if needed, to update the 'check_Key_Status' state)\"\"\"\n",
    "    # BOTH descriptions works. The commented one is optimal when you need to check more key_state, without writing them one by one \n",
    ")\n",
    "\n",
    "search_tool = StructuredTool.from_function(\n",
    "    func=search_place,\n",
    "    name='search_place',\n",
    "    description=\"Searches for a place; Only returns valid data when wifi is on and low_battery_mode is off.\" # Con questa description, il modello verifica direttamente il setting tool prima di invocare il search_tool\n",
    "    # description='A search tool that helps find places or locations' # Con questa description invece il modello solitamente invoca search_tool al primo turn, e di conseguenza aggiusta i settings dove occorre.\n",
    ")\n",
    "\n",
    "tools = [settings_tool, search_tool]\n",
    "\n",
    "tool_descriptions = \"\\n\".join(\n",
    "    [f\"- {t.name}({', '.join(t.args_schema.__annotations__.keys())}) -> {t.description}\"\n",
    "     for t in tools]\n",
    ") # describe the tools, we add it in state_dependency_system as a source of tools data\n",
    "\n",
    "# print('tool_descriptions here =',tool_descriptions)\n",
    "\n",
    "# =======================================\n",
    "# 3) Creazione SystemMessage\n",
    "# ======================================= \n",
    "\n",
    "state_dependency_system = f\"\"\"\n",
    "You are a conversational assistant embedded in a mobile device. The user is likely interacting visually with the screen. Avoid excessive explanations and focus on highlighting only key information. Be factual, clear, and efficient. The user recently returned from international travel, is sleep-deprived, and prefers late-day activities. They have a strong preference for Asian food and often refer to their yoga instructor Lara as ‚Äúmy guide.‚Äù When suggesting options or actions, incorporate these preferences naturally without explicit reminders.\n",
    "Maintain a confident, friendly, and respectful tone. Avoid formal or overly scripted responses. The assistant should behave like a competent personal secretary: efficient, light-toned, and responsive, capable of inferring context when it is clear but cautious where ambiguity exists. Never use pleasantries like ‚Äúplease‚Äù or ‚Äúthank you,‚Äù and avoid robotic phrasing.\n",
    "\n",
    "1) - Do not assume tool parameters unless provided by the user, the prompt history, or the system context.\n",
    "\n",
    "2) - Understand the user intends and call the rigth tools.\n",
    "\n",
    "3) - BEFORE calling any tool: produce a short HUMAN-FACING plan sentence prefixed with \"assistant_plan:\" describing what you will do next (example: \"assistant_plan: i'll check the device settings and enable Wi-Fi if needed, then search for a location\").\n",
    "\n",
    "4) - AFTER tool outputs or after state changes, produce a clear HUMAN-FACING summary prefixed with \"assistant_summary:\" describing the result and next step (example: \"assistant_summary: Wi-Fi was off ‚Üí I turned it on. Now I'll retry the search.\").\n",
    "\n",
    "5) - Use ISO 8601 for any timestamp if needed.\n",
    "\n",
    "6) - NEVER autput chain-of-thought. The plan and summary are the only \"thinking\" visible to the user.\n",
    "\n",
    "7) - Whenever a user ask a question, Respond conversionally but always follow rules nr 3 and 4 around tool usage.\n",
    "\n",
    "8) - Whenever the user asks about a place or location, ALWAYS re-run the `search_place` tool to ensure up-to-date data, even if a previous result exists in memory.\n",
    "\n",
    "9) - Check the stored system settings in memory before making any decision. Only call set_system_settings if the desired setting is not already correct.\n",
    "\n",
    "10) - When the user asks about a place, location, or address (e.g., 'where is', 'find', 'locate', 'search for', etc.), you must always call the `search_place` tool, regardless of memory state or previous results.\n",
    "\n",
    "11) - Never assume a previous search result is still valid; every new user query must trigger a new search tool call.\n",
    "\n",
    "\n",
    "You have access to the following tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "When the user's request clearly matches one of these actions, \n",
    "call the appropriate tool by reasoning about its name and description..\n",
    "\n",
    "\"\"\"\n",
    "# When the user gives an instruction, decide if it‚Äôs:\n",
    "# - A general chat ‚Üí reply with advice, no tool use.\n",
    "\n",
    "# IMPORTANTI DIRETTIVE\n",
    "# senza questa istruzione, il modello e propenso a dare nomi degli args errati (tipa low_battery_status invece di low_battery_mode)\n",
    "# - search_place tool returns valid search data only if 'wifi' is 'on' and low_battery_mode is 'off'. \n",
    "\n",
    "# 1) - Do not assume tool parameters unless provided by the user, the prompt history, or the system context. Questa direttiva ci serve perche le args non le deve decidere il modello, ma sono quelle, a differenza della task Error_recovery dove il modello deve scegliere\n",
    "# se inserire altre args per trovare un evento(e.g. 'add date upperbound'(broadening)) oppure ometterle (e.g. omit date lowerbound (narrowing))\n",
    "\n",
    "# 8) - Whenever the user asks about a place or location, always re-run the `search_place` tool to ensure up-to-date data, even if a previous result exists in memory.\n",
    "# 10) - When the user asks about a place, location, or address (e.g., 'where is', 'find', 'locate', 'search for', etc.), you must always call the `search_place` tool, regardless of memory state or previous results.\n",
    "# 11) - Never assume a previous search result is still valid; every new user query must trigger a new search tool call.\n",
    "# Questa direttive sono importanti perche se per esempio chiediamo di nuovo 'Where is googleplex?' il modello risponde con i dati in memoria della stessa domanda\n",
    "# precedente. Mentre se usiamo questa direttiva, forziamo il modello a chiamare sempre la funzione anche se abbiamo fatto la stessa domanda identica\n",
    "# Questo serve ad avere output di dati dal tool e non dalla memoria(molto importante)\n",
    "\n",
    "# 9) - Check the stored system settings in memory before making any decision. Only call set_system_settings if the desired setting is not already correct.\n",
    "# Questa direttiva e' molto importante perche avendo i dati salvati in memoria non andremo a chiamare inutilmente una tool se non e' necessario (come descritto nella guideline)\n",
    "\n",
    "# =======================================\n",
    "# 3) Creazione del Prompt Template\n",
    "# =======================================\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(state_dependency_system), # The systeMessage\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # chat_history (niettare contenuti dinamici nel prompt.)\n",
    "    MessagesPlaceholder(variable_name=\"system_context\"), # system_context (niettare contenuti dinamici nel prompt.)\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") # 'agent_scratchpad' √à il meccanismo chiave per il reasoning step-by-step del modello.\n",
    "    # Contiene i risultati intermedi (tool call + return) della sessione corrente, cio√®:\n",
    "    # Grazie ad esso il modello apprende anche dal 'return' delle tool invocate, e non solo dalle systemMessage.\n",
    "    # Per esempio, in questo caso quando invochiamo la search tool, il return informa il modello che \"wifi deve essere on e lower_setting_state deve essere off\"\n",
    "    # e grazie a questo return informativo(che ricordiamo non deve essere troppo lungo), l'agent_scratchpad aiuta il modello a capire di agiornare i settings prima di invocare \n",
    "    # nuovamente il 'search tool'. \n",
    "    # In parole semplici: \n",
    "    # \"il modello ‚Äúvede‚Äù che la causa dell‚Äôerrore √® il Wi-Fi spento e pu√≤ decidere da solo di chiamare set_system_settings() per sistemarlo.\"\"\n",
    "])\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "# 4 Creazione della classe SafeConversationMemory(ConversationBufferMemory)\n",
    "\n",
    "class SafeConversationMemory(ConversationBufferMemory): # questa classe viene invocata ogni volta l utente interagisce con il modello, ma non viene ricreata da zero\n",
    "    # viene riutilizzata come istanza persistente, e al suo interno la chat history e gli stati vengono aggiornati.\n",
    "    # ogni volta che comunichiamo con il modello: agent_executor.invoke({\"input\": \"accendi il Wi-Fi\"}), LangChain fa questo dietro le quinte:\n",
    "\n",
    "    # 1) Chiama memory.load_memory_variables() (viene chiamato prima che l'agente costruice il prompt. \n",
    "    # Serve per elaborare i dati in memoria e servirli al modello prima che esso da' una risposta)\n",
    "    \n",
    "    # Serve per:\n",
    "    \n",
    "    # leggere la chat history precedente,\n",
    "    \n",
    "    # leggere eventuali stati (_state_actions),\n",
    "    \n",
    "    # costruire un contesto strutturato in chiave BaseMessage tipo:\n",
    "#     {\n",
    "#   \"chat_history\": [HumanMessage(...), AIMessage(...)],\n",
    "#   \"system_context\": [HumanMessage(content=\"{'wifi': False, 'low_battery_mode': True}\")]\n",
    "#    } üëâ Questo viene poi iniettato nel prompt grazie a MessagesPlaceholder (insieme al system message e all‚Äôinput utente).\n",
    "\n",
    "# 2) L‚ÄôLLM genera una risposta (eventualmente invocando tool)\n",
    "\n",
    "# 3) dopo ogni risposta del modello viene chiamata la funzione 'memory.save_context' che non vediamo qua ma avviene di sottobanco e appartiene ConversationBufferMemory.\n",
    "     # Questa funzione 'aggiorna' il HumanMessage, AIMessage e system_context, in modo che alla prossima domanda fatta al modello, questo possa \n",
    "     # prendere esempio dalle domande, risposte e system_context data del turno precedente (per esempio dopo il primo turn, memory.save_context salva gli stati aggiornati di\n",
    "     # wifi e low_battery_mode ). E' al prossimo turn 'memory.save_context' aggiorna memory.load_memory_variables() e poi il processo si ripete\n",
    "     # dal punto 1) quando lo user pone un altra domanda al modello\n",
    "\n",
    "# Entrambi i metodi che hai citato (save_context() e load_memory_variables()) appartengono alla classe base BaseMemory di LangChain, \n",
    "# e sono implementati concretamente nella sottoclasse ConversationBufferMemory (e quindi anche nella tua SafeConversationMemory)\n",
    "\n",
    "# 'load_memory_variables' viene chiamato prima che l'agente costruice il prompt (cosi prepariamo il modello a tenere conto dei dati della memory prima di leggere promp). \n",
    "# 'save_context' viene chiamato dopo che il modello ha risposto(cosi aggiorniamo per il prossimo turn, e 'load_memory_variables' prepara il modello con i nuovi dati aggiornati da 'memory.save_context' dopo che il modello ha risposto)\n",
    "    \n",
    "    \"\"\"\n",
    "    questa classe estende ConversationBufferMemory, con lo scopo di renderla piu' ‚Äúintelligente‚Äù sicura.\n",
    "    Enhanced memory that:\n",
    "    - Cleans malformed message content (list/dict -> string)\n",
    "    - Keeps track of system_settings status\n",
    "    - Add the actual status into the model through context by adding it in memory, so in the systemMessage you tell the model to verify in memory to make clever decisions \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print('Hit self, **kwargs')\n",
    "       \n",
    "        self._state_actions = {'Key_Status':{ # default status. Per far capire al modello che deve updatare questi date per accedere a serch tool\n",
    "            'wifi':False,\n",
    "            'low_battery_mode':True\n",
    "        }} # This ll be updated when we model calls set_state_action, (we can see updates when invoking memory data).\n",
    "        # Serve a memorizzare lo stato reale del sistema (non solo le parole nella chat).\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # üß† 1Ô∏è‚É£ Methods to manage pending actions\n",
    "    # ============================================================\n",
    "\n",
    "    def set_state_action(self, name: str, key:str, value:str|bool):# ( data: self, name: str, data:dict) # we call this def from the tool def create_reservation(name,data)\n",
    "        \"\"\"Store a pending action waiting for user confirmation.\"\"\"\n",
    "        # self._state_actions[name] = data\n",
    "        self._state_actions[name][key] = value # queste value e key saranno sempre le stesse 2('wifi' e 'low_batteri_mode') e per questo non si sommeranno, ma cambieranno solo le value (true or false) \n",
    "        # Vengono aggiornate uno alla volta, in ordine del modello quando invoka l'update delle key value nella tool set_system_settings\n",
    "\n",
    "    def get_state_action(self, name: str): # controls if there are any pending actions when the model calls add_to_calender(name) Tool\n",
    "        \"\"\"Retrieve a specific pending action.\"\"\"\n",
    "        return self._state_actions.get(name) # invocando questa funzione avremo l attuale status delle key 'wifi' e 'low_batteri_mode'\n",
    "\n",
    "    # ============================================================\n",
    "    # üßπ 2Ô∏è‚É£ Override: cleaning + injecting context for the model\n",
    "    # ============================================================\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Return cleaned chat history + pending actions as context.\"\"\"\n",
    "        raw = super().load_memory_variables(inputs) # il nome del method della classe ConversationBufferMemory, che qui estendiamo e modifichiamo per fare \n",
    "        # pulizie e aggiungere system_context in formato BaseMessage. (deve avere questo nome load_memory_variables)\n",
    "\n",
    "        print('hit load_memory_variables and inputs :',inputs) # {'input': 'where is googleplex?'}\n",
    "        # ---- Clean malformed messages (lists/dicts converted to strings)\n",
    "        if \"chat_history\" in raw:  # chat_history e' la key che abbiamo dato al nostro memoryKey quando abbiamo definito 'memory' e nel prompt\n",
    "            cleaned = []\n",
    "            print('hit chat_history')\n",
    "            for msg in raw[\"chat_history\"]:\n",
    "                if isinstance(msg, BaseMessage): # BaseMessage(HumanMessage(content:'user_input') and AIMessage(content:'ai response')) is the rigth type for our model. Its the type the model require to work without errors.\n",
    "                    # It is like that:\n",
    "                    # {'chat_history': [HumanMessage(content='I need a reservation', additional_kwargs={}, response_metadata={}), AIMessage(content='Could you please clarify what type of reservation you need? For example, is it for a restaurant, hotel, or something else? Additionally, please provide any specific details like the date, time, and number of people if applicable.', additional_kwargs={}, response_metadata={})}\n",
    "                    # so basically our memory stored in ConversationBufferMemory is stored in this way showed in HumanMessage and AIMessage\n",
    "                    msg.content = str(msg.content) # turn (list, dict) content in str type to not throw errors\n",
    "                    cleaned.append(msg)\n",
    "                elif isinstance(msg, str): # if there are some 'fully string type' value that can cause errors(like not structured baseMessage data), instead of BaseMessage structure in our\n",
    "                      # ConversationBufferMemory chat_history , then turn this string  to HumanMessage(). This 'string' bug can hapend when the model calls a tool, creating a \n",
    "                      # 'string with the name of the tool invoked', that the Model doesnt support and cannot parse.\n",
    "                      #(HumanMessage is part of baseMessage for related to the user_input. there is the exemple right up in 'if isinstance(msg, BaseMessage)'). \n",
    "                     cleaned.append(HumanMessage(content=msg)) \n",
    "            raw[\"chat_history\"] = cleaned # puliamo l object chat_history\n",
    "            print('check raw ->', raw)\n",
    "\n",
    "        # ---- Add pending actions as extra conversational context. Questo previene chiamate al tool \"reservation\" quando invokiamo il tool add_to_calender\n",
    "        # inseriamo in memoria che c'e un pending tool ogni volta che il tool \"reservation\" viene invocato, e nel systemMessage diciamo specificamente\n",
    "        # al modello di non invocare nuovamente il tool \"reservation\" quando decidiamo di mettere questo \"reservatio event\" come evento nel tool \"calender\"\n",
    "        # infine ogni volta che il modello inserisce l'evento sul tool calender, il pendig state ritorna vuoto, cosicche' il processo si ripete e \n",
    "        # non ci ripeta lo stesso problema\n",
    "        if self._state_actions: # if _state_actions not empty, cioe' se ci sono dei pending_state \n",
    "            context_str = (\n",
    "                f\"(State actions currently stored: {self._state_actions})\" # questo va a' finire nella memory nel formato humaMessage come bediamo sotto\n",
    "            )\n",
    "            print('check state_actions ->', context_str)\n",
    "        else:\n",
    "            context_str = \"(No state actions stored.)\"\n",
    "        \n",
    "        # aggiungiamo pending_actions in formato HumanMessage\n",
    "        raw[\"system_context\"] = [HumanMessage(content=context_str)] # aggiungiamo un messaggio extra che contiene i dati di ogni turn delle values dei settngs\n",
    "        # sotto il prefisso 'system_context'(inventato da me) che poi inseriamo nel prompt/AIMessagePlaceHolder, cosi ad ogni turno il modello tiene \n",
    "        # conto dei dati ogiornati o meno di settings del turno precedente\n",
    "        # Lo stesso vale per chat_history\n",
    "        # trasformiamo il pending action in uno HumanMessage in modo che il formato sia\n",
    "        # corretto da inserire nella memory e da essere compatimile con il formato del modello.\n",
    "        # Ora  Il modello ‚Äúvede‚Äù il pending_action nel contesto e pu√≤ comportarsi di conseguenza (nei punti 5 e 6 del systemMessage diciamo al\n",
    "        # di guardare i pending_state della memory e cosa fare se ce ne sono di attivi. \n",
    "        \n",
    "        return raw  # ritorniamo la struttura di ConversationBufferMemory() piu' pulita e' con l aggiunt di pending_actions\n",
    "\n",
    "\n",
    "memory = SafeConversationMemory( # istanza della classe SafeConversationMemory. Qui passiamo anche questi parametri definiti qua dentro alla classe\n",
    "    memory_key=\"chat_history\", # crea il BaseMessage della cronologia tra user e model(HumanMessage,AiMessage)\n",
    "    return_messages=True,\n",
    "    input_key=\"input\"  # üëà forza un solo input\n",
    ")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# ‚úÖ  Creazione modello\n",
    "# =======================================\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# ‚úÖ  Creazione agente\n",
    "# =======================================\n",
    "\n",
    "agent = create_openai_functions_agent( # creiamo il nostro agente strutturato\n",
    "    llm=llm,\n",
    "    tools = tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools= tools, \n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"where is googleplex?\" #\"Aggiungi una sessione di stretching alle 7:30.\"\n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondo turn: il modello perche i parametri wifi e lower_battery_status sono salvati corretti in in memoria e quindi decide che non serve \n",
    "# richiamare la search_place(), e risponde ‚ÄúIl Googleplex si trova a Mountain View‚Ä¶‚Äù basandosi solo sulla memoria.\n",
    "\n",
    "user_input = \"where is California?\" #\"Aggiungi una sessione di stretching alle 7:30.\"\n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63222f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"tnx chat\" #\"Aggiungi una sessione di stretching alle 7:30.\"\n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b04ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Ok, where is Googleplex?\" #\"Aggiungi una sessione di stretching alle 7:30.\"\n",
    "response = agent_executor.invoke({\"input\": user_input}) # input is where you add the question, \n",
    "print(response[\"output\"]) # output where you get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4942b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e735cb1",
   "metadata": {},
   "source": [
    "## ONLY TESTES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920ea46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5545628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b581e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wifi = False\n",
    "test = True\n",
    "\n",
    "def set_system_settings(check_Key_Status:str,update_Key_Status:bool|str=None):\n",
    "    \"\"\"Check wifi and battery status\"\"\" \n",
    "    check_Key_Status = check_Key_Status.lower()\n",
    "    print('itemKey here', check_Key_Status)\n",
    "     \n",
    "    getSettingsData = {\n",
    "      \"metadata\": {\n",
    "        \"flow\": {\n",
    "           \"initial_databases\": {\n",
    "                \"SETTING\": [\n",
    "                    {\n",
    "                        \"device_id\": \"04640c30-ec89-5a3c-9fb4-4af3101c8e04\",\n",
    "                        \"place_id\": \"ChIJ_Yjh6Za1j4AR8IgGUZGDDTs\",\n",
    "                        \"latitude\": 37.33464379999999,\n",
    "                        \"longitude\": -122.008972,\n",
    "                        \"formatted_address\": \"One Apple Park Way, Cupertino, CA 95014, USA\",\n",
    "                         \"wifi\": False, # update_Key_Status if 'wifi' in check_Key_Status and update_Key_Status else False,\n",
    "                        \"location_service\": False,\n",
    "                        \"cellular\": False,\n",
    "                        \"low_battery_mode\": True,\n",
    "                        \"utc_offset_seconds\": -25200,\n",
    "                        \"locale\": \"en_US\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    \n",
    "    data_value = getSettingsData['metadata']['flow']['initial_databases']['SETTING'][0][check_Key_Status] # in future, handle error if keyWord is unmatched \n",
    "    if not update_Key_Status: \n",
    "       return F\" {check_Key_Status} status = {data_value}\" # deve essere oltre lo if se vuoi che questo return avviene entro queste contizioni\n",
    "    else:\n",
    "       if check_Key_Status == 'wifi':\n",
    "          global wifi # global has to be above\n",
    "          print('wifi in first funvtion before',wifi)\n",
    "          wifi = update_Key_Status\n",
    "          print('wifi in first funztion after',wifi)\n",
    "          return F\" {check_Key_Status} status = {data_value}\"\n",
    "       else:\n",
    "          print(f'check_Key_Status not wifi: {check_Key_Status} ')\n",
    "          return \"not wifi\"\n",
    "    return \"data\"\n",
    "\n",
    "\n",
    "    \n",
    "def search_place():\n",
    "    \"\"\"Imitating a search tools that invoke goole maps\"\"\"\n",
    "    set_system_settings('wifi',True)\n",
    "    print('wifi here', wifi)\n",
    "       \n",
    "    getPlaceData = [\n",
    "      [\n",
    "    {\n",
    "      \"formatted_address\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043\",\n",
    "      \"geometry\": {\n",
    "        \"location\": {\n",
    "          \"lat\": 37.4220541,\n",
    "          \"lng\": -122.0853242\n",
    "        }\n",
    "      },\n",
    "      \"name\": \"Googleplex\",\n",
    "      \"place_id\": \"ChIJj61dQgK6j4AR4GeTYWZsKWw\",\n",
    "      \"rating\": 4.2,\n",
    "      \"types\": [\n",
    "        \"point_of_interest\",\n",
    "        \"establishment\"\n",
    "      ]\n",
    "     }\n",
    "    ]\n",
    "   ]\n",
    "    return getPlaceData[0][0]['formatted_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search_place())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17484faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
